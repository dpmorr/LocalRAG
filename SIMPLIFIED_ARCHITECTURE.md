# AI Career Mentor â€“ Simplified Microservices Architecture

> Pragmatic microservices approach: Split only where it makes sense for scaling, deployment, and team autonomy.

---

## ğŸ¯ Core Principle

**Split services by resource requirements and scaling needs, not by every domain boundary.**

---

## ğŸ—ï¸ Architecture (4 Core Services)

### Service Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (Next.js)                    â”‚
â”‚                   Glassmorphism UI + Chat                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Service (FastAPI)                    â”‚
â”‚  â€¢ Auth (JWT)                    â€¢ CV parse/critique/gen   â”‚
â”‚  â€¢ Rate limiting                 â€¢ Learning plans           â”‚
â”‚  â€¢ Thread/message management     â€¢ Profile & memory         â”‚
â”‚  â€¢ Document upload               â€¢ PDF rendering            â”‚
â”‚  â€¢ Web retrieval (constrained)   â€¢ Notifications            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                  â”‚
          â–¼                   â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Knowledge Svc   â”‚  â”‚  Inference Svc   â”‚  â”‚  Worker Service  â”‚
â”‚   (FastAPI)      â”‚  â”‚    (vLLM)        â”‚  â”‚    (Celery)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚â€¢ Upload/parse    â”‚  â”‚â€¢ LLM inference   â”‚  â”‚â€¢ Doc processing  â”‚
â”‚â€¢ Chunk/embed     â”‚  â”‚  (Qwen/Llama)    â”‚  â”‚â€¢ Embeddings      â”‚
â”‚â€¢ Vector search   â”‚  â”‚â€¢ Embeddings      â”‚  â”‚â€¢ Memory consolidnâ”‚
â”‚â€¢ BM25 + pgvector â”‚  â”‚  (BGE-M3)        â”‚  â”‚â€¢ Scheduled tasks â”‚
â”‚â€¢ Reranking       â”‚  â”‚â€¢ Reranker        â”‚  â”‚                  â”‚
â”‚                  â”‚  â”‚â€¢ Claude fallback â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚         Shared Infrastructure              â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ â€¢ PostgreSQL (+ pgvector)                  â”‚
         â”‚ â€¢ Redis (cache + Celery broker)            â”‚
         â”‚ â€¢ S3/MinIO (documents, artifacts)          â”‚
         â”‚ â€¢ Jaeger (tracing)                         â”‚
         â”‚ â€¢ Prometheus + Grafana (metrics)           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Service Details

### 1. **Frontend Service** (Next.js)
**Port**: 3000
**Tech**: Next.js 14, React, Tailwind, Framer Motion
**Responsibility**: UI only, no business logic

**Why separate**:
- Independent frontend deploys
- CDN distribution (CloudFront)
- Different scaling needs (CPU vs GPU)

---

### 2. **API Service** (FastAPI)
**Port**: 8080
**Tech**: FastAPI, LangGraph, SQLAlchemy, Celery client
**Responsibility**: All business logic, orchestration, tools

**Modules**:
```
api-service/
â”œâ”€â”€ auth/           # JWT, session management
â”œâ”€â”€ chat/           # LangGraph orchestrator, threads
â”œâ”€â”€ cv/             # CV parsing, critique, generation
â”œâ”€â”€ plan/           # Learning path builder
â”œâ”€â”€ profile/        # User profiles, memory
â”œâ”€â”€ upload/         # File upload handling
â”œâ”€â”€ render/         # PDF/diagram generation (Puppeteer)
â”œâ”€â”€ web/            # Constrained web scraping
â””â”€â”€ notifications/  # Email, in-app alerts
```

**Why keep together**:
- Shares same database (easy transactions)
- Tool coordination is simpler
- No network overhead between tools
- Single deployment for business logic

---

### 3. **Knowledge Service** (FastAPI)
**Port**: 8081
**Tech**: FastAPI, pgvector, BM25 indexing
**Responsibility**: Document pipeline + retrieval

**Functions**:
- Document upload â†’ parse â†’ normalize
- Chunking (512-1024 tokens)
- Coordinate with Inference Service for embeddings
- Hybrid search (BM25 + vector)
- Reranking

**Why separate**:
- Heavy I/O (file processing, DB writes)
- Independent scaling (more uploads = more instances)
- Can queue work to Workers without blocking API

---

### 4. **Inference Service** (vLLM)
**Port**: 8000 (OpenAI-compatible)
**Tech**: vLLM serving
**Responsibility**: GPU inference only

**Models**:
- Primary: Qwen2.5-14B (or Llama 3.1-8B)
- Embeddings: BGE-M3
- Reranker: BGE-reranker-v2-m3
- Fallback: Claude/Gemini via API (in API Service)

**Why separate**:
- **GPU resources** (expensive, needs dedicated nodes)
- **Independent scaling** (GPU autoscaling)
- **Stateless** (easy horizontal scaling)
- **Upgradeable** (swap models without touching business logic)

---

### 5. **Worker Service** (Celery)
**Port**: N/A (background)
**Tech**: Celery + Redis broker
**Responsibility**: Async/scheduled jobs

**Tasks**:
- Parse uploaded documents
- Generate embeddings (calls Inference Service)
- Index to vector DB (calls Knowledge Service)
- Memory consolidation (nightly)
- Evaluation runs (scheduled)

**Why separate**:
- Long-running tasks (don't block API)
- Independent scaling (queue depth-based)
- Retry/failure handling

---

## ğŸ—„ï¸ Data Architecture

### PostgreSQL Schemas
```sql
-- Single database, multiple schemas for organization
career_mentor/
  â”œâ”€â”€ auth          # users, sessions, tokens
  â”œâ”€â”€ chat          # threads, messages, runs, checkpoints
  â”œâ”€â”€ knowledge     # documents, chunks, embeddings, citations
  â”œâ”€â”€ profiles      # user_profiles, memories, preferences
  â””â”€â”€ artifacts     # generated_cvs, plans, exports
```

**Why single DB**:
- Easier transactions (e.g., create thread + update profile)
- Simpler migrations
- Lower operational overhead
- Can split later if needed (start simple)

### Redis Usage
```
redis/
  â”œâ”€â”€ cache:*           # Search results, embeddings (TTL)
  â”œâ”€â”€ session:*         # User sessions (TTL)
  â”œâ”€â”€ ratelimit:*       # Rate limit counters (TTL)
  â””â”€â”€ celery:*          # Task queue
```

### S3 Buckets
```
s3://career-mentor/
  â”œâ”€â”€ raw/              # Uploaded docs
  â”œâ”€â”€ clean/            # Normalized markdown
  â”œâ”€â”€ chunks/           # Parquet chunks with metadata
  â”œâ”€â”€ artifacts/        # PDFs, diagrams
  â””â”€â”€ templates/        # CV/plan templates
```

---

## ğŸ”„ Communication Patterns

### Synchronous (REST)
```
Frontend â†’ API Service â†’ Knowledge Service (search)
                      â†’ Inference Service (generate)
```

### Asynchronous (Celery + Redis)
```
API Service â†’ enqueue task â†’ Worker â†’ calls Knowledge/Inference
```

### Example Flow: Upload Document
```
1. User uploads PDF â†’ API Service
2. API Service:
   - Saves to S3 (raw/)
   - Enqueues: parse_document_task(doc_id)
   - Returns: {doc_id, status: "processing"}
3. Worker picks up task:
   - Downloads from S3
   - Parses to markdown
   - Chunks text
   - Calls Inference Service (embed)
   - Calls Knowledge Service (index)
   - Updates DB status: "ready"
4. API Service notifies user (WebSocket or polling)
```

### Example Flow: Chat Message
```
1. User sends message â†’ API Service
2. API Service (LangGraph):
   a. Retrieve context â†’ Knowledge Service.search()
   b. Generate answer â†’ Inference Service (POST /v1/chat/completions)
   c. Update memory â†’ local DB write
   d. Return message + citations
3. Frontend renders message
```

---

## ğŸš€ Deployment

### Development (Docker Compose)
```bash
docker-compose up
# Starts: postgres, redis, api-service, knowledge-service,
#         inference-service, worker, jaeger, grafana
```

### Production (Simple)

**Option A: Single VPS/EC2** (cheapest, easiest)
```
- 1x Large Instance (e.g., g5.2xlarge with GPU)
- Docker Compose or K3s (lightweight Kubernetes)
- Nginx reverse proxy
- Managed Postgres (RDS) + Redis (ElastiCache)
```

**Option B: Kubernetes (EKS/GKE)** (if scaling needed)
```
Namespaces:
  - app (api-service, knowledge-service)
  - inference (inference-service on GPU nodes)
  - workers (worker pods, CPU autoscaling)

Node Pools:
  - General (c6i.2xlarge): API, Knowledge, Workers
  - GPU (g5.2xlarge): Inference Service only
```

---

## ğŸ“Š Why This Split Works

| Service | Scaling Need | Resource Type | Deploy Frequency |
|---------|--------------|---------------|------------------|
| **Frontend** | High (traffic) | CPU, CDN | Often (UI changes) |
| **API Service** | Medium | CPU, Memory | Often (features) |
| **Knowledge** | Medium | I/O, CPU | Medium (indexing) |
| **Inference** | Low-Medium | GPU | Rare (model updates) |
| **Workers** | Elastic | CPU | Medium (tasks) |

âœ… **Easy to deploy**: 4 services vs 13
âœ… **Easy to develop**: Less inter-service coordination
âœ… **Easy to debug**: Fewer network hops
âœ… **Cost effective**: Share resources where possible
âœ… **Still scalable**: Split the bottlenecks (GPU, I/O, compute)

---

## ğŸ”§ Technology Stack

| Layer | Technology |
|-------|------------|
| **Frontend** | Next.js 14, React, Tailwind, Framer Motion |
| **API** | FastAPI, LangGraph, LangChain, SQLAlchemy |
| **Knowledge** | FastAPI, pgvector, BM25 (pg_trgm) |
| **Inference** | vLLM (Qwen2.5/Llama), BGE-M3 |
| **Workers** | Celery, Python |
| **Database** | PostgreSQL 15 + pgvector |
| **Cache/Queue** | Redis 7 |
| **Storage** | MinIO (dev), S3 (prod) |
| **Tracing** | OpenTelemetry + Jaeger |
| **Metrics** | Prometheus + Grafana |
| **IaC** | Terraform (optional) or Docker Compose |

---

## ğŸ“‚ Simplified Repository Structure

```
career-mentor/
â”œâ”€â”€ frontend/                   # Next.js app
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/                    # Main API service
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ cv/
â”‚   â”‚   â”œâ”€â”€ plan/
â”‚   â”‚   â”œâ”€â”€ profile/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ knowledge/              # Knowledge service
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ workers/                # Celery workers
â”‚       â”œâ”€â”€ tasks/
â”‚       â”œâ”€â”€ celery_app.py
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ docker-compose.yml      # Local dev
â”‚   â”œâ”€â”€ k8s/                    # Kubernetes manifests (optional)
â”‚   â””â”€â”€ terraform/              # IaC (optional)
â”œâ”€â”€ docs/
â”œâ”€â”€ tests/
â””â”€â”€ README.md
```

**Note**: Inference service uses pre-built vLLM Docker image (no custom code needed).

---

## ğŸ¯ Migration Path

### Start (MVP)
```
Everything in API Service (monolith)
â†“
Extract GPU inference â†’ Inference Service
â†“
Extract heavy I/O â†’ Knowledge Service
â†“
Add background jobs â†’ Workers
```

### If Growth Requires
```
Split API Service:
  â†’ Auth Service (if multi-tenancy/SSO needed)
  â†’ Chat Service (if chat scaling is bottleneck)
```

**Start simple, split only when needed.**

---

## âœ… Acceptance Criteria (Simplified)

- [ ] 4 services deployable with `docker-compose up`
- [ ] Frontend accessible at `localhost:3000`
- [ ] API docs at `localhost:8080/docs`
- [ ] Upload doc â†’ searchable in <2 min
- [ ] Chat returns answer with citations in <5s
- [ ] CV critique completes in <30s
- [ ] All services instrumented (traces in Jaeger)
- [ ] Health checks pass (`/health` endpoints)
- [ ] Can deploy to single GPU instance or K8s

---

## ğŸ“ Next Steps

1. **Bootstrap repos** (mono-repo or multi-repo)
2. **Set up dev environment** (`docker-compose up`)
3. **Implement API Service** (auth, chat, tools)
4. **Implement Knowledge Service** (upload, search)
5. **Configure vLLM** (download model, test inference)
6. **Add Workers** (async doc processing)
7. **Build Frontend** (glassmorphism UI)
8. **Add observability** (Jaeger, Prometheus)
9. **Write tests** (integration, e2e)
10. **Deploy** (VPS or K8s)

---

**This is a pragmatic, production-ready architecture that's actually deployable without a DevOps team.**
